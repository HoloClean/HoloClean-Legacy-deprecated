{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create grounding** - We begin by reading our data set from csv file and put it in the spark dataframe . \n",
    "**Get Data** - we read data from a csv file which each tuple known with its index and list of denial constarints\n",
    "**Prepare Data** - We parse each denial constraints and to make SQL query over it \n",
    "**Analyze Data** - For each constraint we make a dataframe that contains tuple that satisfy that denial constraint      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ***pyspark*** library is used for all the data analysis excluding a small piece of the data presentation section. The ***numpy*** library will only be needed for set functionality. Importing the libraries is the first step we will take in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all libraries needed for the tutorial\n",
    "\n",
    "# General syntax to import specific functions in a library: \n",
    "##from (library) import (specific library function)\n",
    "import pyspark as ps #this is how I usually import spark\n",
    "import numpy as np #this is how I usually import numpy\n",
    "import sys #For some system calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Spark version 2.2.0\n",
      "numpy version 1.12.1\n"
     ]
    }
   ],
   "source": [
    "print('Python version ' + sys.version)\n",
    "print('Spark version ' + ps.__version__)\n",
    "print('numpy version ' + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark Session  \n",
    "\n",
    "The spark session for this tutorial runs locally and its name is \"Grounding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The inital set of baby names and bith rates\n",
    "spark = ps.sql.SparkSession.builder.master(\"local\").appName(\"Word Count\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data \n",
    "\n",
    "In this section we use spark session to read csv file which its name is 10.csv that include 10 weather data with header index , city , temp (which is the temperature) , tempType (the temperature type TMIN or TMAX) and date ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+--------+----+\n",
      "|index|       city|    date|tempType|temp|\n",
      "+-----+-----------+--------+--------+----+\n",
      "|    1|ITE00100554|18000101|    TMAX| -75|\n",
      "|    2|ITE00100554|18000101|    TMIN|-148|\n",
      "|    3|GM000010962|18000101|    PRCP|   0|\n",
      "|    4|EZE00100082|18000101|    TMAX| -86|\n",
      "|    5|EZE00100082|18000101|    TMAX|-135|\n",
      "|    6|ITE00100554|18000102|    TMAX| -60|\n",
      "|    7|ITE00100554|18000102|    TMIN|-125|\n",
      "|    8|GM000010962|18000102|    PRCP|   0|\n",
      "|    9|EZE00100082|18000102|    TMAX| -44|\n",
      "|   10|EZE00100082|18000102|    TMIN|-135|\n",
      "+-----+-----------+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"10.csv\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are basically done reading the data set. \n",
    "\n",
    "***df*** will be a ***DataFrame*** object. You can think of this object holding the contents of the BabyDataSet in a format similar to a sql table or an excel spreadsheet. In next step we get get the list of denial constraints normally denial constraints make a logical compare between two tuples\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "\\forall t_1,t_2 : \\lbrace P_1 \\cap P_2 \\cap ... \\cap  P_m  \\rbrace\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We use similar format to show the denial constraints that used in Holostic Error Detection algorithm we also have a list of noisy cells which is not needed for this part.For simplicity we assume that we just just one denial constraint. It says that in one city if one year we have maximum temerature and that temprature cannot be minimum tempreture for same city.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcCode=['t1&t2&EQ(t1.city,t2.city)&EQ(t1.temp,t2.temp)&IQ(t1.tempType,t2.tempType)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grounding use for making feature vectors for learning part of holoclean to this end we have a class for parsing the input data and make some SQL predicate for making query over our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenialConstraint:\n",
    "\n",
    "    operationsArr=['=' , '<' , '>' , '<>' , '<=' ,'>=']\n",
    "    operationSign=['EQ','LT', 'GT','IQ','LTE', 'GTE']\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self,denial_constraints):\n",
    "        self.denial_constraints=denial_constraints\n",
    "\n",
    "    def dc2SqlCondition(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates list of list of sql predicates by parsing the input denial constraints\n",
    "        :return: list[list[string]]\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        dcSql=[]\n",
    "        usedOperations=[]\n",
    "        numOfContraints=len(self.denial_constraints)\n",
    "        for i in range(0,numOfContraints):\n",
    "            ruleParts=self.denial_constraints[i].split('&')\n",
    "            firstTuple=ruleParts[0]\n",
    "            secondTuple=ruleParts[1]\n",
    "            numOfpredicate=len(ruleParts)-2\n",
    "            dcOperations=[]\n",
    "            dc2sqlpred=[]\n",
    "            for c in range(2,len(ruleParts)):\n",
    "                dc2sql=''\n",
    "                predParts=ruleParts[c].split('(')\n",
    "                op=predParts[0]\n",
    "                dcOperations.append(op)\n",
    "                predBody=predParts[1][:-1]\n",
    "                tmp=predBody.split(',')\n",
    "                predLeft=tmp[0]\n",
    "                predRight=tmp[1]\n",
    "                #predicate type detection\n",
    "                if firstTuple in predBody and secondTuple in predBody:\n",
    "                    if firstTuple in predLeft:\n",
    "                        dc2sql= dc2sql+'table1.'+ predLeft.split('.')[1]+ self.operationsArr[self.operationSign.index(op)]+'table2.'+predRight.split('.')[1]\n",
    "                    else:\n",
    "                        dc2sql= dc2sql+'table2.'+ predLeft.split('.')[1]+ self.operationsArr[self.operationSign.index(op)]+'table1.'+predRight.split('.')[1]\n",
    "                elif firstTuple in predBody:\n",
    "                    if firstTuple in predLeft:\n",
    "                        dc2sql= dc2sql+'table1.'+ predLeft.split('.')[1]+ self.operationsArr[self.operationSign.index(op)]+predRight\n",
    "                    else:\n",
    "                        dc2sql= dc2sql+ predLeft+ self.operationsArr[self.operationSign.index(op)]+'table1.'+ predRight.split('.')[1]\n",
    "                else:\n",
    "                    if secondTuple in predLeft:\n",
    "                        dc2sql= dc2sql+'table2.'+ predLeft.split('.')[1]+ self.operationsArr[self.operationSign.index(op)]+predRight\n",
    "                    else:\n",
    "                        dc2sql= dc2sql+ predLeft+ self.operationsArr[self.operationSign.index(op)]+'table2.'+ predRight.split('.')[1]\n",
    "                dc2sqlpred.append(dc2sql)\n",
    "            usedOperations.append(dcOperations)\n",
    "            dcSql.append(dc2sqlpred) \n",
    "        return dcSql,usedOperations   \n",
    "    \n",
    "    def make_and_condition(self,conditionInd):\n",
    "        \"\"\"\n",
    "        return the list of indexed constraints\n",
    "        :param conditionInd: int\n",
    "        :return: string\n",
    "        \"\"\"\n",
    "        result,dc=self.dc2SqlCondition()\n",
    "        parts=result[conditionInd]\n",
    "        strRes=str(parts[0])\n",
    "        if len(parts)>1:\n",
    "            for i in range(1,len(parts)):\n",
    "                strRes=strRes+\" AND \"+str(parts[i])\n",
    "        return strRes\n",
    "    \n",
    "    def noViolation_tuple(self,dataset,condition,spak_session):\n",
    "        \n",
    "        dataset.createOrReplaceTempView(\"df\")   \n",
    "        q=\"SELECT table1.index as indexT1,table2.index as indexT2 FROM df table1,df table2 WHERE NOT(\"+ self. make_and_condition(condition)+\")\"        \n",
    "        satisfied_tuples_index=spak_session.sql(q)         \n",
    "        return satisfied_tuples_index\n",
    "    \n",
    "    def violation_tuple(self,dataset,condition,spak_session):\n",
    "        \n",
    "        dataset.createOrReplaceTempView(\"df\")   \n",
    "        q=\"SELECT table1.index as indexT1,table2.index as indexT2 FROM df table1,df table2 WHERE (\"+ self. make_and_condition(condition)+\")\"        \n",
    "        not_satisfied_tuples_index=spak_session.sql(q)         \n",
    "        return not_satisfied_tuples_index\n",
    "    \n",
    "    def all_dc_violation(self,dataset,spak_session):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return list of violation tuples dataframe\n",
    "        :param dataset: Dataframe\n",
    "        :param spak_session: SparkSession        \n",
    "        :return: list[Dataframe]\n",
    "        \"\"\"\n",
    "        \n",
    "        return [self.violation_tuple(dataset, i, spak_session) for i in range(0,len(self.denial_constraints))]\n",
    "    \n",
    "    def all_dc_nonViolation(self,dataset,spak_session):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return list of non violation tuples dataframe\n",
    "        :param dataset: Dataframe\n",
    "        :param spak_session: SparkSession        \n",
    "        :return: list[Dataframe]\n",
    "        \"\"\"\n",
    "        \n",
    "        return [self.noViolation_tuple(dataset, i, spak_session) for i in range(0,len(self.denial_constraints))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we can test method $dc2SqlCondition$ by creating an object from DenialConstraint and give $dcCode$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t1&t2&EQ(t1.city,t2.city)&EQ(t1.temp,t2.temp)&IQ(t1.tempType,t2.tempType)']\n",
      "[['table1.city=table2.city', 'table1.temp=table2.temp', 'table1.tempType<>table2.tempType']]\n",
      "table1.city=table2.city AND table1.temp=table2.temp AND table1.tempType<>table2.tempType\n",
      "+-----+-----------+--------+--------+----+\n",
      "|index|       city|    date|tempType|temp|\n",
      "+-----+-----------+--------+--------+----+\n",
      "|    1|ITE00100554|18000101|    TMAX| -75|\n",
      "|    2|ITE00100554|18000101|    TMIN|-148|\n",
      "|    3|GM000010962|18000101|    PRCP|   0|\n",
      "|    4|EZE00100082|18000101|    TMAX| -86|\n",
      "|    5|EZE00100082|18000101|    TMAX|-135|\n",
      "|    6|ITE00100554|18000102|    TMAX| -60|\n",
      "|    7|ITE00100554|18000102|    TMIN|-125|\n",
      "|    8|GM000010962|18000102|    PRCP|   0|\n",
      "|    9|EZE00100082|18000102|    TMAX| -44|\n",
      "|   10|EZE00100082|18000102|    TMIN|-135|\n",
      "+-----+-----------+--------+--------+----+\n",
      "\n",
      "None\n",
      "%%%%%%%%%%%%%%%%%%%%%%\n",
      "SELECT table1.index as indexT1,table2.index as indexT2 FROM df table1,df table2 WHERE NOT(table1.city=table2.city AND table1.temp=table2.temp AND table1.tempType<>table2.tempType AND table1.temp=table2.temp AND table1.tempType<>table2.tempType)\n",
      "98\n",
      "+-------+-------+\n",
      "|indexT1|indexT2|\n",
      "+-------+-------+\n",
      "|      1|      1|\n",
      "|      1|      2|\n",
      "|      1|      3|\n",
      "|      1|      4|\n",
      "|      1|      5|\n",
      "|      1|      6|\n",
      "|      1|      7|\n",
      "|      1|      8|\n",
      "|      1|      9|\n",
      "|      1|     10|\n",
      "|      2|      1|\n",
      "|      2|      2|\n",
      "|      2|      3|\n",
      "|      2|      4|\n",
      "|      2|      5|\n",
      "|      2|      6|\n",
      "|      2|      7|\n",
      "|      2|      8|\n",
      "|      2|      9|\n",
      "|      2|     10|\n",
      "|      3|      1|\n",
      "|      3|      2|\n",
      "|      3|      3|\n",
      "|      3|      4|\n",
      "|      3|      5|\n",
      "|      3|      6|\n",
      "|      3|      7|\n",
      "|      3|      8|\n",
      "|      3|      9|\n",
      "|      3|     10|\n",
      "|      4|      1|\n",
      "|      4|      2|\n",
      "|      4|      3|\n",
      "|      4|      4|\n",
      "|      4|      5|\n",
      "|      4|      6|\n",
      "|      4|      7|\n",
      "|      4|      8|\n",
      "|      4|      9|\n",
      "|      4|     10|\n",
      "|      5|      1|\n",
      "|      5|      2|\n",
      "|      5|      3|\n",
      "|      5|      4|\n",
      "|      5|      5|\n",
      "|      5|      6|\n",
      "|      5|      7|\n",
      "|      5|      8|\n",
      "|      5|      9|\n",
      "|      6|      1|\n",
      "|      6|      2|\n",
      "|      6|      3|\n",
      "|      6|      4|\n",
      "|      6|      5|\n",
      "|      6|      6|\n",
      "|      6|      7|\n",
      "|      6|      8|\n",
      "|      6|      9|\n",
      "|      6|     10|\n",
      "|      7|      1|\n",
      "|      7|      2|\n",
      "|      7|      3|\n",
      "|      7|      4|\n",
      "|      7|      5|\n",
      "|      7|      6|\n",
      "|      7|      7|\n",
      "|      7|      8|\n",
      "|      7|      9|\n",
      "|      7|     10|\n",
      "|      8|      1|\n",
      "|      8|      2|\n",
      "|      8|      3|\n",
      "|      8|      4|\n",
      "|      8|      5|\n",
      "|      8|      6|\n",
      "|      8|      7|\n",
      "|      8|      8|\n",
      "|      8|      9|\n",
      "|      8|     10|\n",
      "|      9|      1|\n",
      "|      9|      2|\n",
      "|      9|      3|\n",
      "|      9|      4|\n",
      "|      9|      5|\n",
      "|      9|      6|\n",
      "|      9|      7|\n",
      "|      9|      8|\n",
      "|      9|      9|\n",
      "|      9|     10|\n",
      "|     10|      1|\n",
      "|     10|      2|\n",
      "|     10|      3|\n",
      "|     10|      4|\n",
      "|     10|      6|\n",
      "|     10|      7|\n",
      "|     10|      8|\n",
      "|     10|      9|\n",
      "|     10|     10|\n",
      "+-------+-------+\n",
      "\n",
      "None\n",
      "%%%%%%%%%%%%%%%%%%%%%%\n",
      "2\n",
      "+-------+-------+\n",
      "|indexT1|indexT2|\n",
      "+-------+-------+\n",
      "|      5|     10|\n",
      "|     10|      5|\n",
      "+-------+-------+\n",
      "\n",
      "None\n",
      "%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    }
   ],
   "source": [
    "print(dcCode)\n",
    "dc=DenialConstraint(dcCode)\n",
    "predicatePart,l=dc.dc2SqlCondition()\n",
    "print(predicatePart)\n",
    "print(dc. make_and_condition(0))\n",
    "print (df.show(df.count()))\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "df.createOrReplaceTempView(\"df\") \n",
    "multiCond=\"SELECT table1.index as indexT1,table2.index as indexT2 FROM df table1,df table2 WHERE NOT(\"+ dc.make_and_condition(0) +\" AND \"+ str(predicatePart[0][1])+\" AND \"+ str(predicatePart[0][2])+\")\"\n",
    "print(multiCond)\n",
    "singleCond=\"SELECT table1.index as indexT1,table2.index as indexT2 FROM df table1,df table2 WHERE (\"+ dc. make_and_condition(0) +\")\"\n",
    "mcDF=spark.sql(multiCond)\n",
    "scDF=spark.sql(singleCond)\n",
    "print(mcDF.count())\n",
    "print (mcDF.show(mcDF.count()))\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(scDF.count())\n",
    "print (scDF.show(scDF.count()))\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if the predicate have n part then it will make a list of length n and put each predicate in order. Method (make_condition) just return list that in position conditionInd\n",
    "\n",
    "\n",
    "## Finding satisfied tuples using Spark\n",
    "\n",
    "In this section we make method for finding tuple that shows the indices record of data that satisfy the specified constraint for this we have noViolation_tuple that make dataframe with columns that eash row shows which two indices are satisfies that denial constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|indexT1|indexT2|\n",
      "+-------+-------+\n",
      "|      1|      1|\n",
      "|      1|      2|\n",
      "|      1|      3|\n",
      "|      1|      4|\n",
      "|      1|      5|\n",
      "|      1|      6|\n",
      "|      1|      7|\n",
      "|      1|      8|\n",
      "|      1|      9|\n",
      "|      1|     10|\n",
      "|      2|      1|\n",
      "|      2|      2|\n",
      "|      2|      3|\n",
      "|      2|      4|\n",
      "|      2|      5|\n",
      "|      2|      6|\n",
      "|      2|      7|\n",
      "|      2|      8|\n",
      "|      2|      9|\n",
      "|      2|     10|\n",
      "|      3|      1|\n",
      "|      3|      2|\n",
      "|      3|      3|\n",
      "|      3|      4|\n",
      "|      3|      5|\n",
      "|      3|      6|\n",
      "|      3|      7|\n",
      "|      3|      8|\n",
      "|      3|      9|\n",
      "|      3|     10|\n",
      "|      4|      1|\n",
      "|      4|      2|\n",
      "|      4|      3|\n",
      "|      4|      4|\n",
      "|      4|      5|\n",
      "|      4|      6|\n",
      "|      4|      7|\n",
      "|      4|      8|\n",
      "|      4|      9|\n",
      "|      4|     10|\n",
      "|      5|      1|\n",
      "|      5|      2|\n",
      "|      5|      3|\n",
      "|      5|      4|\n",
      "|      5|      5|\n",
      "|      5|      6|\n",
      "|      5|      7|\n",
      "|      5|      8|\n",
      "|      5|      9|\n",
      "|      6|      1|\n",
      "|      6|      2|\n",
      "|      6|      3|\n",
      "|      6|      4|\n",
      "|      6|      5|\n",
      "|      6|      6|\n",
      "|      6|      7|\n",
      "|      6|      8|\n",
      "|      6|      9|\n",
      "|      6|     10|\n",
      "|      7|      1|\n",
      "|      7|      2|\n",
      "|      7|      3|\n",
      "|      7|      4|\n",
      "|      7|      5|\n",
      "|      7|      6|\n",
      "|      7|      7|\n",
      "|      7|      8|\n",
      "|      7|      9|\n",
      "|      7|     10|\n",
      "|      8|      1|\n",
      "|      8|      2|\n",
      "|      8|      3|\n",
      "|      8|      4|\n",
      "|      8|      5|\n",
      "|      8|      6|\n",
      "|      8|      7|\n",
      "|      8|      8|\n",
      "|      8|      9|\n",
      "|      8|     10|\n",
      "|      9|      1|\n",
      "|      9|      2|\n",
      "|      9|      3|\n",
      "|      9|      4|\n",
      "|      9|      5|\n",
      "|      9|      6|\n",
      "|      9|      7|\n",
      "|      9|      8|\n",
      "|      9|      9|\n",
      "|      9|     10|\n",
      "|     10|      1|\n",
      "|     10|      2|\n",
      "|     10|      3|\n",
      "|     10|      4|\n",
      "|     10|      6|\n",
      "|     10|      7|\n",
      "|     10|      8|\n",
      "|     10|      9|\n",
      "|     10|     10|\n",
      "+-------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dp=DenialConstraint(dcCode)\n",
    "gh=dp.noViolation_tuple(df, 0, spark)\n",
    "cnt=gh.count()\n",
    "print(gh.show(cnt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in dataframe index 10 and 5 doesnt satisfiy the denial costraint the city is paris and the minimum temperature in one year is equal to maximum temperature in another year so this is error and as you can see in the tple dataframe neither (10,5) and (5,10) didn't appeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+--------+----+\n",
      "|index|       city|    date|tempType|temp|\n",
      "+-----+-----------+--------+--------+----+\n",
      "|    1|ITE00100554|18000101|    TMAX| -75|\n",
      "|    2|ITE00100554|18000101|    TMIN|-148|\n",
      "|    3|GM000010962|18000101|    PRCP|   0|\n",
      "|    4|EZE00100082|18000101|    TMAX| -86|\n",
      "|    5|EZE00100082|18000101|    TMAX|-135|\n",
      "|    6|ITE00100554|18000102|    TMAX| -60|\n",
      "|    7|ITE00100554|18000102|    TMIN|-125|\n",
      "|    8|GM000010962|18000102|    PRCP|   0|\n",
      "|    9|EZE00100082|18000102|    TMAX| -44|\n",
      "|   10|EZE00100082|18000102|    TMIN|-135|\n",
      "+-----+-----------+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding not satisfied tuples using Spark\n",
    "\n",
    "In this section we make method for finding tuples that shows the indices of records of data that not satisfy the we specified constraint.For this we have violation_tuple that make dataframe with columns that eash row shows which two indices didn't satisfy the specified denial constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|indexT1|indexT2|\n",
      "+-------+-------+\n",
      "|      5|     10|\n",
      "|     10|      5|\n",
      "+-------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dp=DenialConstraint(dcCode)\n",
    "gh=dp.violation_tuple(df, 0, spark)\n",
    "cnt=gh.count()\n",
    "print(gh.show(cnt)) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

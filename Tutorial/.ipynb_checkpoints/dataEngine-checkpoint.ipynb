{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Learning Data** - We begin by reading our data set from csv file and put it in the spark dataframe and use signal to make feature vectors\n",
    "**Get Data** - we read data from a csv file which each tuple known with its index and list of denial constarints\n",
    "**Prepare Data** - We parse each denial constraints and to make SQL query over it \n",
    "**Analyze Data** - For each constraint we make a dataframe that contains tuple that satisfy that denial constraint      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ***pyspark*** library is used for all the data analysis excluding a small piece of the data presentation section. The ***numpy*** library will only be needed for set functionality. Importing the libraries is the first step we will take in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all libraries needed for the tutorial\n",
    "\n",
    "# General syntax to import specific functions in a library: \n",
    "##from (library) import (specific library function)\n",
    " \n",
    "import dataEngine as de#we testing this module\n",
    "import pyspark as ps #this is how I usually import spark\n",
    "import numpy as np #this is how I usually import numpy\n",
    "import sys #For some system calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Spark version 2.2.0\n",
      "numpy version 1.12.1\n"
     ]
    }
   ],
   "source": [
    "print('Python version ' + sys.version)\n",
    "print('Spark version ' + ps.__version__)\n",
    "print('numpy version ' + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark Session  \n",
    "\n",
    "The spark session for this tutorial runs locally and its name is \"Grounding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The inital set of baby names and bith rates\n",
    "spark = ps.sql.SparkSession.builder.master(\"local\").appName(\"Data Featurizing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data \n",
    "\n",
    "In this section we use spark session to read csv file which its name is 10.csv that include 10 weather data with header index , city , temp (which is the temperature) , tempType (the temperature type TMIN or TMAX) and date ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+--------+----+\n",
      "|index|       city|    date|tempType|temp|\n",
      "+-----+-----------+--------+--------+----+\n",
      "|    1|ITE00100554|18000101|    TMAX| -75|\n",
      "|    2|ITE00100554|18000101|    TMIN|-148|\n",
      "|    3|GM000010962|18000101|    PRCP|   0|\n",
      "|    4|EZE00100082|18000101|    TMAX| -86|\n",
      "|    5|EZE00100082|18000101|    TMAX|-135|\n",
      "|    6|ITE00100554|18000102|    TMAX| -60|\n",
      "|    7|ITE00100554|18000102|    TMIN|-125|\n",
      "|    8|GM000010962|18000102|    PRCP|   0|\n",
      "|    9|EZE00100082|18000102|    TMAX| -44|\n",
      "|   10|EZE00100082|18000102|    TMIN|-135|\n",
      "+-----+-----------+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"10.csv\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenialConstraint Class \n",
    "This module parse denial constraints and make query for finding tuples that satisfy the a specific constraint. We choose that all denial constraints have structure as shown in below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcCode=['t1&t2&EQ(t1.city,t2.city)&EQ(t1.temp,t2.temp)&IQ(t1.tempType,t2.tempType)']\n",
    "noisy_cells=[('5','city'),('1','temp')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we can test method $dc2SqlCondition$ by creating an object from DenialConstraint and give $dcCode$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of lists that containt predicates for each DC :\n",
      "\n",
      "sql pred of t1&t2&EQ(t1.city,t2.city)&EQ(t1.temp,t2.temp)&IQ(t1.tempType,t2.tempType) is : \n",
      "['table1.city=table2.city', 'table1.temp=table2.temp', 'table1.tempType<>table2.tempType']\n",
      "and its operations:\n",
      "['EQ', 'EQ', 'IQ']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dcObj=de.DenialConstraint(dcCode)\n",
    "set_of_preds,operations=dcObj.dc2SqlCondition()\n",
    "print (\"List of lists that containt predicates for each DC :\\n\")\n",
    "for i in range(0,len(set_of_preds)):\n",
    "    print (\"sql pred of \" + dcCode[i] + \" is : \")\n",
    "    print (set_of_preds[i])\n",
    "    print (\"and its operations:\")\n",
    "    print (operations[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if the predicate have n part then it will make a list of length n and put each predicate in order. Method (make_condition) just return list that in position conditionInd\n",
    "\n",
    "\n",
    "### Finding satisfied tuples using Spark\n",
    "\n",
    "In this section we make method for finding tuple that shows the indices record of data that satisfy the specified constraint for this we have noViolation_tuple that make dataframe with columns that eash row shows which two indices are satisfies that denial constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|indexT1|indexT2|\n",
      "+-------+-------+\n",
      "|      1|      1|\n",
      "|      1|      2|\n",
      "|      1|      3|\n",
      "|      1|      4|\n",
      "|      1|      5|\n",
      "|      1|      6|\n",
      "|      1|      7|\n",
      "|      1|      8|\n",
      "|      1|      9|\n",
      "|      1|     10|\n",
      "|      2|      1|\n",
      "|      2|      2|\n",
      "|      2|      3|\n",
      "|      2|      4|\n",
      "|      2|      5|\n",
      "|      2|      6|\n",
      "|      2|      7|\n",
      "|      2|      8|\n",
      "|      2|      9|\n",
      "|      2|     10|\n",
      "|      3|      1|\n",
      "|      3|      2|\n",
      "|      3|      3|\n",
      "|      3|      4|\n",
      "|      3|      5|\n",
      "|      3|      6|\n",
      "|      3|      7|\n",
      "|      3|      8|\n",
      "|      3|      9|\n",
      "|      3|     10|\n",
      "|      4|      1|\n",
      "|      4|      2|\n",
      "|      4|      3|\n",
      "|      4|      4|\n",
      "|      4|      5|\n",
      "|      4|      6|\n",
      "|      4|      7|\n",
      "|      4|      8|\n",
      "|      4|      9|\n",
      "|      4|     10|\n",
      "|      5|      1|\n",
      "|      5|      2|\n",
      "|      5|      3|\n",
      "|      5|      4|\n",
      "|      5|      5|\n",
      "|      5|      6|\n",
      "|      5|      7|\n",
      "|      5|      8|\n",
      "|      5|      9|\n",
      "|      6|      1|\n",
      "|      6|      2|\n",
      "|      6|      3|\n",
      "|      6|      4|\n",
      "|      6|      5|\n",
      "|      6|      6|\n",
      "|      6|      7|\n",
      "|      6|      8|\n",
      "|      6|      9|\n",
      "|      6|     10|\n",
      "|      7|      1|\n",
      "|      7|      2|\n",
      "|      7|      3|\n",
      "|      7|      4|\n",
      "|      7|      5|\n",
      "|      7|      6|\n",
      "|      7|      7|\n",
      "|      7|      8|\n",
      "|      7|      9|\n",
      "|      7|     10|\n",
      "|      8|      1|\n",
      "|      8|      2|\n",
      "|      8|      3|\n",
      "|      8|      4|\n",
      "|      8|      5|\n",
      "|      8|      6|\n",
      "|      8|      7|\n",
      "|      8|      8|\n",
      "|      8|      9|\n",
      "|      8|     10|\n",
      "|      9|      1|\n",
      "|      9|      2|\n",
      "|      9|      3|\n",
      "|      9|      4|\n",
      "|      9|      5|\n",
      "|      9|      6|\n",
      "|      9|      7|\n",
      "|      9|      8|\n",
      "|      9|      9|\n",
      "|      9|     10|\n",
      "|     10|      1|\n",
      "|     10|      2|\n",
      "|     10|      3|\n",
      "|     10|      4|\n",
      "|     10|      6|\n",
      "|     10|      7|\n",
      "|     10|      8|\n",
      "|     10|      9|\n",
      "|     10|     10|\n",
      "+-------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "noVio=dcObj.noViolation_tuple(df, 0, spark)\n",
    "cnt=noVio.count()\n",
    "print(noVio.show(cnt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in dataframe index 10 and 5 doesnt satisfiy the denial costraint the city is paris and the minimum temperature in one year is equal to maximum temperature in another year so this is error and as you can see in the tple dataframe neither (10,5) and (5,10) didn't appeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+--------+----+\n",
      "|index|       city|    date|tempType|temp|\n",
      "+-----+-----------+--------+--------+----+\n",
      "|    1|ITE00100554|18000101|    TMAX| -75|\n",
      "|    2|ITE00100554|18000101|    TMIN|-148|\n",
      "|    3|GM000010962|18000101|    PRCP|   0|\n",
      "|    4|EZE00100082|18000101|    TMAX| -86|\n",
      "|    5|EZE00100082|18000101|    TMAX|-135|\n",
      "|    6|ITE00100554|18000102|    TMAX| -60|\n",
      "|    7|ITE00100554|18000102|    TMIN|-125|\n",
      "|    8|GM000010962|18000102|    PRCP|   0|\n",
      "|    9|EZE00100082|18000102|    TMAX| -44|\n",
      "|   10|EZE00100082|18000102|    TMIN|-135|\n",
      "+-----+-----------+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding not satisfied tuples using Spark\n",
    "\n",
    "In this section we make method for finding tuples that shows the indices of records of data that not satisfy the we specified constraint.For this we have violation_tuple that make dataframe with columns that eash row shows which two indices didn't satisfy the specified denial constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|indexT1|indexT2|\n",
      "+-------+-------+\n",
      "|      5|     10|\n",
      "|     10|      5|\n",
      "+-------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vio=dcObj.violation_tuple(df, 0, spark)\n",
    "cnt=vio.count()\n",
    "print(vio.show(cnt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DomainPruning Class\n",
    "In this section we try to check the domain pruning part of the code. We have 2 dirty cells in this example and for each of them first we calculate the feasible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The domain for attribute ', 'city', ' is ', [u'EZE00100082'])\n",
      "('The domain for attribute ', 'temp', ' is ', [u'-75'])\n"
     ]
    }
   ],
   "source": [
    "dp=de.DomainPruning(df,noisy_cells)\n",
    "dVal=dp.candidate_values()\n",
    "for i in range(0,len(dVal)):\n",
    "    print(\"The domain for attribute \",noisy_cells[i][1],\" is \",dVal[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next method we have make dictionary of lists that show domain for each attribute in the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': [u'EZE00100082'],\n",
       " 'date': [u'18000101', u'18000102'],\n",
       " 'temp': [u'-75'],\n",
       " 'tempType': [u'PRCP', u'TMAX', u'TMIN']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.allowable_doamin_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other methods like allowable row that return dataframe from  that is are line without noisy cells and its value are in the domain after pruning. that will help when we want to choose training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use other function when we make featurized table\n",
    "\n",
    "## QuantativeStatisticsFeaturize Class\n",
    "\n",
    "This class make feature vector from statistical signals for cells in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/anaconda2/lib/python2.7/site-packages/pyspark/sql/session.py:331: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cell', 'freq_2_city_date', 'freq_2_city_temp', 'freq_2_city_tempType', 'freq_2_date_temp', 'freq_2_date_tempType', 'freq_2_tempType_temp', 'freq_3_city_date_temp', 'freq_3_city_date_tempType', 'freq_3_city_tempType_temp', 'freq_3_date_tempType_temp', 'freq_city', 'freq_date', 'freq_temp', 'freq_tempType']\n",
      "+------------+----------------+----------------+--------------------+----------------+--------------------+--------------------+---------------------+-------------------------+-------------------------+-------------------------+---------+---------+---------+-------------+\n",
      "|        cell|freq_2_city_date|freq_2_city_temp|freq_2_city_tempType|freq_2_date_temp|freq_2_date_tempType|freq_2_tempType_temp|freq_3_city_date_temp|freq_3_city_date_tempType|freq_3_city_tempType_temp|freq_3_date_tempType_temp|freq_city|freq_date|freq_temp|freq_tempType|\n",
      "+------------+----------------+----------------+--------------------+----------------+--------------------+--------------------+---------------------+-------------------------+-------------------------+-------------------------+---------+---------+---------+-------------+\n",
      "|    [1,city]|               2|               1|                   2|               0|                   0|                   0|                    1|                        1|                        1|                        0|        4|        0|        0|            0|\n",
      "|    [1,date]|               2|               0|                   0|               1|                   3|                   0|                    1|                        1|                        0|                        1|        0|        5|        0|            0|\n",
      "|[1,tempType]|               0|               0|                   2|               0|                   3|                   1|                    0|                        1|                        1|                        1|        0|        0|        0|            5|\n",
      "|    [1,temp]|               0|               1|                   0|               1|                   0|                   1|                    1|                        0|                        1|                        1|        0|        0|        1|            0|\n",
      "|    [2,city]|               2|               1|                   2|               0|                   0|                   0|                    1|                        1|                        1|                        0|        4|        0|        0|            0|\n",
      "|    [2,date]|               2|               0|                   0|               1|                   1|                   0|                    1|                        1|                        0|                        1|        0|        5|        0|            0|\n",
      "|[2,tempType]|               0|               0|                   2|               0|                   1|                   1|                    0|                        1|                        1|                        1|        0|        0|        0|            3|\n",
      "|    [2,temp]|               0|               1|                   0|               1|                   0|                   1|                    1|                        0|                        1|                        1|        0|        0|        1|            0|\n",
      "|    [3,city]|               1|               2|                   2|               0|                   0|                   0|                    1|                        1|                        2|                        0|        2|        0|        0|            0|\n",
      "|    [3,date]|               1|               0|                   0|               1|                   1|                   0|                    1|                        1|                        0|                        1|        0|        5|        0|            0|\n",
      "|[3,tempType]|               0|               0|                   2|               0|                   1|                   2|                    0|                        1|                        2|                        1|        0|        0|        0|            2|\n",
      "|    [3,temp]|               0|               2|                   0|               1|                   0|                   2|                    1|                        0|                        2|                        1|        0|        0|        2|            0|\n",
      "|    [4,city]|               2|               1|                   3|               0|                   0|                   0|                    1|                        2|                        1|                        0|        4|        0|        0|            0|\n",
      "|    [4,date]|               2|               0|                   0|               1|                   3|                   0|                    1|                        2|                        0|                        1|        0|        5|        0|            0|\n",
      "|[4,tempType]|               0|               0|                   3|               0|                   3|                   1|                    0|                        2|                        1|                        1|        0|        0|        0|            5|\n",
      "|    [4,temp]|               0|               1|                   0|               1|                   0|                   1|                    1|                        0|                        1|                        1|        0|        0|        1|            0|\n",
      "|    [5,city]|               2|               2|                   3|               0|                   0|                   0|                    1|                        2|                        1|                        0|        4|        0|        0|            0|\n",
      "|    [5,date]|               2|               0|                   0|               1|                   3|                   0|                    1|                        2|                        0|                        1|        0|        5|        0|            0|\n",
      "|[5,tempType]|               0|               0|                   3|               0|                   3|                   1|                    0|                        2|                        1|                        1|        0|        0|        0|            5|\n",
      "|    [5,temp]|               0|               2|                   0|               1|                   0|                   1|                    1|                        0|                        1|                        1|        0|        0|        2|            0|\n",
      "+------------+----------------+----------------+--------------------+----------------+--------------------+--------------------+---------------------+-------------------------+-------------------------+-------------------------+---------+---------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "qsObj=de.QuantativeStatisticsFeaturize(df)\n",
    "qsFeatureVectors=qsObj.featurize(spark)\n",
    "print(qsFeatureVectors.columns)\n",
    "print(qsFeatureVectors.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenialConstraintFeaturize Class\n",
    "\n",
    "This class make feature vector from denial constraints signals for cells in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dcfObj=de.DenialConstraintFeaturize(df,dcCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For featurize for denial constraints we need list of all satisfied tuple this list come from DenialConstraint class we have its object just call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|        cell|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10|\n",
      "+------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|    [1,city]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [1,date]|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|[1,tempType]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [1,temp]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [2,city]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [2,date]|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|[2,tempType]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [2,temp]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [3,city]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [3,date]|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|[3,tempType]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [3,temp]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [4,city]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [4,date]|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|[4,tempType]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [4,temp]|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|    [5,city]|  1|  1|  1|  1|  1|  1|  1|  1|  1| -1|\n",
      "|    [5,date]|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|[5,tempType]|  1|  1|  1|  1|  1|  1|  1|  1|  1| -1|\n",
      "|    [5,temp]|  1|  1|  1|  1|  1|  1|  1|  1|  1| -1|\n",
      "+------------+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noviolationList=dcObj.all_dc_nonViolation(df,spark)\n",
    "dcfFeatureVector=dcfObj.featurize(noviolationList,spark)\n",
    "dcfFeatureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HolocleanData Class\n",
    "\n",
    "### Join all signal feature vector\n",
    "This class make aggregation between signals , split feature vector for noisy cells and clean data now we make object and try to aggregate our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cell',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " 'freq_2_city_date',\n",
       " 'freq_2_city_temp',\n",
       " 'freq_2_city_tempType',\n",
       " 'freq_2_date_temp',\n",
       " 'freq_2_date_tempType',\n",
       " 'freq_2_tempType_temp',\n",
       " 'freq_3_city_date_temp',\n",
       " 'freq_3_city_date_tempType',\n",
       " 'freq_3_city_tempType_temp',\n",
       " 'freq_3_date_tempType_temp',\n",
       " 'freq_city',\n",
       " 'freq_date',\n",
       " 'freq_temp',\n",
       " 'freq_tempType']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd=de.HolocleanData(df)\n",
    "joint_table =hd.signal_features_aggregator(dcfFeatureVector,qsFeatureVectors)\n",
    "joint_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split clean cells and noisy cells \n",
    "Next step is seprate the feature vectors of noisy cells and clean cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Whole data', 40)\n",
      "('Noisy cells', 2)\n",
      "('Clean cells', 38)\n"
     ]
    }
   ],
   "source": [
    "noisy_cells_fv,clean_cells_fv=hd.split_noisy_nonNoisy(joint_table,noisy_cells,spark)\n",
    "print(\"Whole data\",joint_table.count())\n",
    "print(\"Noisy cells\",noisy_cells_fv.count())\n",
    "print(\"Clean cells\",clean_cells_fv.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select cells for trainig data\n",
    "In our learning policy we dont use cells that are in the row that has noisy cell we also need to choose data from clean cells that are in the pruned domain For this we might need to prune data from clean cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of clean cells', 38)\n",
      "('Number of selected cells', 19)\n"
     ]
    }
   ],
   "source": [
    "selected_cell=dp.allowable_rows(clean_cells_fv,spark)\n",
    "print (\"Number of clean cells\",clean_cells_fv.count())\n",
    "print (\"Number of selected cells\",selected_cell.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claculate label for training data\n",
    "\n",
    "In the last part of data preparing we need to calculate the label of selected trainig data so we can use another function of HolocleanData class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['city_withValue_ITE00100554',\n",
       " 'city_withValue_GM000010962',\n",
       " 'city_withValue_EZE00100082',\n",
       " 'date_withValue_18000101',\n",
       " 'date_withValue_18000102',\n",
       " 'tempType_withValue_TMAX',\n",
       " 'tempType_withValue_TMIN',\n",
       " 'tempType_withValue_PRCP',\n",
       " 'temp_withValue_-75',\n",
       " 'temp_withValue_-148',\n",
       " 'temp_withValue_0',\n",
       " 'temp_withValue_-86',\n",
       " 'temp_withValue_-135',\n",
       " 'temp_withValue_-60',\n",
       " 'temp_withValue_-125',\n",
       " 'temp_withValue_-44']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_selected_cell=hd.make_trainingdata_label(selected_cell,spark)\n",
    "print(label_selected_cell.count())\n",
    "label_selected_cell.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seprat index column from data\n",
    "We need seprate data index from feature vector by using another function from HolocleanData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           as|\n",
      "+-------------+\n",
      "| [4,tempType]|\n",
      "|     [2,date]|\n",
      "|     [3,date]|\n",
      "|     [8,date]|\n",
      "| [8,tempType]|\n",
      "|     [9,date]|\n",
      "|     [6,date]|\n",
      "|    [10,city]|\n",
      "|    [10,date]|\n",
      "| [3,tempType]|\n",
      "| [9,tempType]|\n",
      "| [6,tempType]|\n",
      "|[10,tempType]|\n",
      "|     [7,date]|\n",
      "| [7,tempType]|\n",
      "| [2,tempType]|\n",
      "|     [4,city]|\n",
      "|     [9,city]|\n",
      "|     [4,date]|\n",
      "+-------------+\n",
      "\n",
      "None\n",
      "('Before sepration', ['cell', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'freq_2_city_date', 'freq_2_city_temp', 'freq_2_city_tempType', 'freq_2_date_temp', 'freq_2_date_tempType', 'freq_2_tempType_temp', 'freq_3_city_date_temp', 'freq_3_city_date_tempType', 'freq_3_city_tempType_temp', 'freq_3_date_tempType_temp', 'freq_city', 'freq_date', 'freq_temp', 'freq_tempType'])\n",
      "('After sepration', ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'freq_2_city_date', 'freq_2_city_temp', 'freq_2_city_tempType', 'freq_2_date_temp', 'freq_2_date_tempType', 'freq_2_tempType_temp', 'freq_3_city_date_temp', 'freq_3_city_date_tempType', 'freq_3_city_tempType_temp', 'freq_3_date_tempType_temp', 'freq_city', 'freq_date', 'freq_temp', 'freq_tempType'])\n"
     ]
    }
   ],
   "source": [
    "index,featureVectors=hd.index_X(selected_cell,spark)\n",
    "print(index.show())\n",
    "print('Before sepration',selected_cell.columns)\n",
    "print(\"After sepration\",featureVectors.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to learning !\n",
    "In the last part we our x and y so we can fed them to TensorFlow"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

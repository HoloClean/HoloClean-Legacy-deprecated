{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Step-by-Step Guide to Holoclean example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy and erroneous data is a major bottleneck in analytics. Data cleaning and repairing account for about 60% of the work of data scientists. To address this bottleneck, we recently introduced HoloClean, a semi-automated data repairing framework that relies on statistical learning and inference to repair errors in structured data. In HoloClean, we build upon the paradigm of weak supervision and demonstrate how to leverage diverse signals, including user-defined heuristic rules (such as generalized data integrity constraints) and external dictionaries, to repair erroneous data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we walk through the process of implementing Holoclean, by creating a simple end-to-end example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import all the module from Holoclean that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from holoclean.holoclean import HoloClean, Session\n",
    "from holoclean.errordetection.errordetector import ErrorDetectors\n",
    "from holoclean.featurization.featurizer import SignalInit, SignalCooccur, SignalDC\n",
    "from holoclean.featurization.featurizer import Featurizer\n",
    "from holoclean.learning.softmax import SoftMax\n",
    "from holoclean.learning.accuracy import Accuracy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Initialization\n",
    "In this part, we create the Holoclean and Session object that we will use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holo_obj = HoloClean()\n",
    "session = Session(\"Session\", holo_obj)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Input and DC from file\n",
    "Test data and the Denial Constraints will be read using the Session's ingestor.\n",
    "After ingesting the test data will be loaded into MySQL tables along with entries in the a metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"test/hospital1k/hospital_dataset.csv\"\n",
    "# dataset = \"test/flights/flights_input_holo.csv\"\"\n",
    "# dataset = \"test/food/food_input_holo.csv\"\n",
    "dataset = \"test/unit_test/unit_test_dataset.csv\"\n",
    "\n",
    "# denial_constraints = \"test/hospital1k/hospital_constraints.txt\"\n",
    "# denial_constraints = \"test/flights/flight_constraints.txt\"\n",
    "# denial_constraints = \"test/food/food_constraints1.txt\"\n",
    "denial_constraints = \"test/unit_test/unit_test_constraints.txt\"\n",
    "\n",
    "# Ingesting Dataset and Denial Constraints\n",
    "start_time = time.time()\n",
    "t0 = time.time()\n",
    "session.ingest_dataset(dataset)\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "\n",
    "\n",
    "print 'time for ingesting file: ' + str(total) + '\\n'\n",
    "session.denial_constraints(denial_constraints)\n",
    "print 'Init table'\n",
    "sql = holo_obj.dataengine.get_table_to_dataframe(\"Init\", session.dataset)\n",
    "#sql.select('index','ProviderNumber','HospitalName', 'Address1').show()\n",
    "\n",
    "sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Detection\n",
    "In this part, we create the error detection. The output of this part is the C_dk table that contains all the noisy cells and the C_Clean table that contains the clean cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "err_detector = ErrorDetectors(session.Denial_constraints, holo_obj.dataengine,\n",
    "                             holo_obj.spark_session, session.dataset)\n",
    "session.add_error_detector(err_detector)\n",
    "session.ds_detect_errors()\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "holo_obj.logger.info('error dectection time: '+str(total)+'\\n')\n",
    "print 'error dectection time: '+str(total)+'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Pruning\n",
    "In this part, we prune the domain. The output of this part is the possible_values tables that contains all the possible values for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "pruning_threshold = 0.5\n",
    "session.ds_domain_pruning(pruning_threshold)\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "holo_obj.logger.info('domain pruning time: '+str(total)+'\\n')\n",
    "print 'domain pruning time: '+str(total)+'\\n'\n",
    "\n",
    "print 'Possible_values_dk'\n",
    "sql = holo_obj.dataengine.get_table_to_dataframe(\"Possible_values_clean\", session.dataset)\n",
    "sql.show()\n",
    "\n",
    "print 'Possible values dk'\n",
    "sql = holo_obj.dataengine.get_table_to_dataframe(\"Possible_values_dk\", session.dataset)\n",
    "sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we implement the featurization module of holoclean. We choose the signals that we want to use and the output of this part is the featurization table that contains the factors that we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "initial_value_signal = SignalInit(session.Denial_constraints, holo_obj.dataengine,\n",
    "                              session.dataset)\n",
    "session.add_featurizer(initial_value_signal )\n",
    "statistics_signal = SignalCooccur(session.Denial_constraints, holo_obj.dataengine,\n",
    "                              session.dataset )\n",
    "session.add_featurizer(statistics_signal)\n",
    "dc_signal = SignalDC(session.Denial_constraints, holo_obj.dataengine, session.dataset,\n",
    "                 holo_obj.spark_session)\n",
    "session.add_featurizer(dc_signal)\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "print \"Feature Signal Time:\", total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the signals that we choose in the previous step. The output of this part is the featurization table that contains the factors that we will use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "session.ds_featurize()\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1 - t0\n",
    "\n",
    "holo_obj.logger.info('featurization time: '+str(total)+'\\n')\n",
    "print 'featurization time: '+str(total)+'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Learning\n",
    "We create the X-tensor from the feature_clean table and run softmax on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "soft = SoftMax(holo_obj.dataengine, session.dataset, holo_obj.spark_session,\n",
    "                       session.X_training)\n",
    "\n",
    "print(soft.logreg())\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "\n",
    "print 'time for training model: '+str(total)+'\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we use the new weight, to learn the probabilities for each value for the cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "session.ds_featurize(0)\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "print 'time for test featurization: ' + str(total) + '\\n'\n",
    "\n",
    "Y = soft.predict(soft.model, soft.setuptrainingX(), soft.setupMask(0))\n",
    "print(Y)\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "print 'time for inference: ', total\n",
    "soft.save_prediction(Y)\n",
    "\n",
    "print 'Inferred values for dk cells'\n",
    "sql = holo_obj.dataengine.get_table_to_dataframe(\"Inferred_values\", session.dataset)\n",
    "sql.show()\n",
    "\n",
    "endtime = time.time()\n",
    "print 'total time: ', endtime - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: A Complete HoloClean Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk step-by-step through the process of repairing a dataset in `HoloClean`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset in question contains information about hospitals and is commonly-used for research purposes. Errors are present in ~5% of the cells and there is significant duplicate information - the ideal environment for `HoloClean`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by instantiating the `HoloClean` and `Session` objects needed to run the repairs. For a more detailed overview of these objects and the rest of our infrastructure, please see our [Data Loading & Denial Constraints Tutorial](Tutorial_1.ipynb).\n",
    "To altar exposed variables such as pruning threshold. \n",
    "Send as a parameter when initializing the HoloClean object.\n",
    "Examples of exposed variables:\n",
    "    Learning Rate: learning_rate\n",
    "    Pruning Threshold: pruning_threshold\n",
    "    Number of Learning Iterations: learning_iterations\n",
    "    Batch Size: batch_size\n",
    "For a list of all possible arguements check the holoclean.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilyas/anaconda3/envs/py27/lib/python2.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from holoclean.holoclean import HoloClean, Session\n",
    "\n",
    "holo       = HoloClean(\n",
    "            holoclean_path=\"..\",         # path to holoclean package\n",
    "            verbose=False,\n",
    "            # to limit possible values for training data\n",
    "            pruning_threshold1=0.1,\n",
    "            # to limit possible values for training data to less than k values\n",
    "            pruning_clean_breakoff=6,\n",
    "            # to limit possible values for dirty data (applied after\n",
    "            # Threshold 1)\n",
    "            pruning_threshold2=0,\n",
    "            # to limit possible values for dirty data to less than k values\n",
    "            pruning_dk_breakoff=6,\n",
    "            # learning parameters\n",
    "            learning_iterations=30,\n",
    "            learning_rate=0.001,\n",
    "            batch_size=5\n",
    "        )\n",
    "session = Session(holo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the data and denial constraints needed for this dataset. Both pieces of information are stored in the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data/hospital.csv\"\n",
    "\n",
    "## loads data into our database and returns pyspark dataframe of initial data\n",
    "data = session.load_data(data_path)\n",
    "\n",
    "dc_path = \"data/hospital_constraints.txt\"\n",
    "\n",
    "# loads denial constraints into our database and returns a simple list of dcs as strings \n",
    "dcs = session.load_denial_constraints(dc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to see the dataset has a decent amount of errors. Note the random 'x' characters that have been substituted in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      City|\n",
      "+----------+\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birminghxm|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingxam|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffxeld|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all pyspark dataframe commands available\n",
    "data.select('City').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t1&t2&EQ(t1.ZipCode,t2.ZipCode)&IQ(t1.City,t2.City)',\n",
       " 't1&t2&EQ(t1.ZipCode,t2.ZipCode)&IQ(t1.State,t2.State)',\n",
       " 't1&t2&EQ(t1.PhoneNumber,t2.PhoneNumber)&IQ(t1.ZipCode,t2.ZipCode)',\n",
       " 't1&t2&EQ(t1.PhoneNumber,t2.PhoneNumber)&IQ(t1.City,t2.City)',\n",
       " 't1&t2&EQ(t1.PhoneNumber,t2.PhoneNumber)&IQ(t1.State,t2.State)',\n",
       " 't1&t2&EQ(t1.ProviderNumber,t2.ProviderNumber)&EQ(t1.MeasureCode,t2.MeasureCode)&IQ(t1.Stateavg,t2.Stateavg)',\n",
       " 't1&t2&EQ(t1.MeasureCode,t2.MeasureCode)&IQ(t1.MeasureName,t2.MeasureName)',\n",
       " 't1&t2&EQ(t1.MeasureCode,t2.MeasureCode)&IQ(t1.Condition,t2.Condition)',\n",
       " 't1&t2&EQ(t1.State,t2.State)&EQ(t1.MeasureCode,t2.MeasureCode)&IQ(t1.Stateavg,t2.Stateavg)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple list of strings\n",
    "dcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Error Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HoloClean is a supervised error repairing system. In contrast to traditional supervision, we do not ask users to label individual data cells but rely on more high-level supervision signals. These signals are denial constraints or other, custom-made error detectors that split the data into two categories, \"clean\" and \"don't-know\". Using that split, our later steps of the process will be able to learn the features of a \"clean\" cell and perform inference on the values of the \"don't-know\" cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see our <a href=http://pages.cs.wisc.edu/~thodrek/blog/holoclean.html>blog post</a> for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will use HoloClean's built in error detector that uses denial constraints to perform this split. Any cell that participates in a violation of a DC is marked \"don't-know\", the rest are treated as clean. If you wish to develop a custom error detector, please see our [Error Detectors Tutorial](Tutorial_3.ipynb) for a walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from holoclean.errordetection.sql_dcerrordetector import SqlDCErrorDetection\n",
    "\n",
    "# instantiate Holoclean's built in error detector\n",
    "detector = SqlDCErrorDetection(session)\n",
    "\n",
    "# both clean and dirty sets are returned as pyspark dataframes\n",
    "error_detector_list =[]\n",
    "error_detector_list.append(detector)\n",
    "clean, dirty = session.detect_errors(error_detector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ind=3, attr=u'Score'),\n",
       " Row(ind=22, attr=u'Sample'),\n",
       " Row(ind=28, attr=u'Address3'),\n",
       " Row(ind=30, attr=u'Address2'),\n",
       " Row(ind=38, attr=u'Sample')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ind=554, attr=u'ZipCode'),\n",
       " Row(ind=387, attr=u'State'),\n",
       " Row(ind=471, attr=u'ZipCode'),\n",
       " Row(ind=703, attr=u'ZipCode'),\n",
       " Row(ind=514, attr=u'State')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Repairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"clean\" and \"don't-know\" split defined, we are ready to perform repairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denial Constraints are the driving force behind this process. Denial constraints are used as features in a softmax regression model. The clean cells are used as training examples to learn the parameters (weights) of this model. Once those weights are defined, we use this model to perform inference on the \"don't-know\" cells and insert the most likely value for each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will simply use the default parameters for our softmax model. Customization of parameters like learning rate, batch size, and number of epochs is described in Tutorial 4 (in development) and is recommended for performance-critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 13.34it/s]\n"
     ]
    }
   ],
   "source": [
    "repaired = session.repair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our repaired dataset has effectively removed large numbers of the 'x' characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      City|\n",
      "+----------+\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "|birmingham|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "| sheffield|\n",
      "+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repaired = repaired.withColumn(\"__ind\", repaired[\"__ind\"].cast(\"int\"))\n",
    "repaired.sort('__ind').select('City').show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a research dataset, a clean version is available for us to compare our results to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.compare_to_truth(\"data/hospital_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

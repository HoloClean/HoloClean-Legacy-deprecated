{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Custom Error Detectors\n",
    "\n",
    "HoloClean learns to clean data by first splitting it into two categories `clean` and `dont_know` or `dk` for short. It then uses the `clean` set to learn a factor graph representation of your data. We've provided one kind of error detector, the `DCErrorDetector` which uses Denial Constraints to make these splits. However, HoloClean accepts arbitrary splits through the `ErrorDetector` class.\n",
    "\n",
    "# A `hello world` Example\n",
    "The heart of an error detector is two functions, `get_noisy_cells` and `get_clean_cells`. We are using the hospital dataset from before. We know that some Zip Codes are formatted incorrectly, so we'll write an error detector that gives HoloClean all the erroneous Zip Codes using some simple regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleErrorDetector:\n",
    "    def __init__(self, session):\n",
    "        self.spark_session = session.holo_env.spark_session\n",
    "        self.dataengine = session.holo_env.dataengine\n",
    "        self.dataset = session.dataset\n",
    "    \n",
    "    def get_noisy_cells(self):\n",
    "        '''\n",
    "            well get a spark DataFrame Instance of our Data\n",
    "            and return a new DataFrame with the schema \n",
    "            |ind|attr|\n",
    "            \n",
    "            where ind is the index of our data \n",
    "            and attr is the name of the column \n",
    "            or columns we believe are dirty\n",
    "        '''\n",
    "        spark_data_frame = self.dataengine.get_table_to_dataframe('Init', self.dataset)\n",
    "    \n",
    "        spark_data_frame.createOrReplaceTempView(\"table1\")\n",
    "        query = \"SELECT __ind as ind \"\\\n",
    "                \"FROM table1 \"\\\n",
    "                \"WHERE \"\\\n",
    "                \"ZipCode NOT RLIKE '[0-9]{5}'\"\n",
    "            \n",
    "        result = self.spark_session.sql(query)\n",
    "        attr_frame = self.spark_session.createDataFrame([['ZipCode']], ['attr'])\n",
    "        result = result.crossJoin(attr_frame)\n",
    "        return result\n",
    "                                              \n",
    "                                      \n",
    "        \n",
    "    \n",
    "    def get_clean_cells(self):\n",
    "        '''\n",
    "            The same as before, but now we'll get \n",
    "            reference noisy data in case we need it\n",
    "        '''\n",
    "        spark_data_frame = self.dataengine.get_table_to_dataframe('Init', self.dataset)\n",
    "        \n",
    "        spark_data_frame.createOrReplaceTempView(\"table1\")\n",
    "        query = \"SELECT __ind as ind \"\\\n",
    "                \"FROM table1 \"\\\n",
    "                \"WHERE \"\\\n",
    "                \"ZipCode RLIKE '[0-9]{5}'\"\n",
    "            \n",
    "        result = self.spark_session.sql(query)\n",
    "        attr_frame = self.spark_session.createDataFrame([['ZipCode']], ['attr'])\n",
    "        result = result.crossJoin(attr_frame)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll start up HoloClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoclean.holoclean import HoloClean, Session\n",
    "\n",
    "holo       =  HoloClean(\n",
    "            holoclean_path=\"..\",         # path to holoclean package\n",
    "            verbose=False,\n",
    "            # to limit possible values for training data\n",
    "            pruning_threshold1=0.1,\n",
    "            # to limit possible values for training data to less than k values\n",
    "            pruning_clean_breakoff=6,\n",
    "            # to limit possible values for dirty data (applied after\n",
    "            # Threshold 1)\n",
    "            pruning_threshold2=0,\n",
    "            # to limit possible values for dirty data to less than k values\n",
    "            pruning_dk_breakoff=6,\n",
    "            # learning parameters\n",
    "            learning_iterations=30,\n",
    "            learning_rate=0.001,\n",
    "            batch_size=5\n",
    "        )\n",
    "session = Session(holo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And ingest the dataset\n",
    "\n",
    "You can review what's happening here in our [Data Loading & Denial Constraints Tutorial](Tutorial_1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"data/hospital.csv\"\n",
    "\n",
    "ground_truth = \"data/hospital_clean.csv\"\n",
    "\n",
    "# Ingesting Dataset and Denial Constraints\n",
    "\n",
    "data = session.load_data(dataset)\n",
    "\n",
    "data.select('__ind','ProviderNumber','HospitalName', 'Address1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Your Error Detector to HoloClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We instantiate an ErrorDetector class,\n",
    "    and give it an instance of our\n",
    "    SimpleErrorDetector Object\n",
    "'''\n",
    "err = SimpleErrorDetector(session)\n",
    "#run error detection\n",
    "error_detector_list =[]\n",
    "error_detector_list.append(err)\n",
    "clean, dirty = session.detect_errors(error_detector_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the results\n",
    "\n",
    "The following table will give us all records which are believed to be erroneous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we view the original dataset, viewing index 45 will confirm our suspicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter(data.__ind == 45).select([\"__ind\",\"ZipCode\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief tutorial explains Holoclean's built-in performance evaluation. At this time, it is a simple process. We input a 'ground truth' dataset with all of the correct values for each cell. This ground truth needs to be created for each dataset that you wish to evaluate Holoclean on. \n",
    "\n",
    "Holoclean then compares its inferred values to the correct values in the ground truth and outputs simple measures of precision and accuracy. Let's see an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code again for running Holoclean on the hospital dataset. See our [Complete Pipeline Tutorial](Tutorial_2.ipynb) if you haven't already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from holoclean.holoclean import HoloClean, Session\n",
    "from holoclean.errordetection.sql_dcerrordetector import SqlDCErrorDetection\n",
    "\n",
    "holo = HoloClean(holoclean_path=\"..\")\n",
    "session = Session(holo)\n",
    "\n",
    "data_path = \"data/hospital.csv\"\n",
    "data = session.load_data(data_path)\n",
    "\n",
    "dc_path = \"data/hospital_constraints.txt\"\n",
    "dcs = session.load_denial_constraints(dc_path)\n",
    "\n",
    "\n",
    "detector = SqlDCErrorDetection(session)\n",
    "\n",
    "error_detector_list =[]\n",
    "error_detector_list.append(detector)\n",
    "clean, dirty = session.detect_errors(error_detector_list)\n",
    "\n",
    "repaired = session.repair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the evaluation as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-1 precision  is : 0.998859934853\n",
      "The top-1 recall is : 0.978378378378 out of 185\n"
     ]
    }
   ],
   "source": [
    "session.compare_to_truth(\"data/hospital_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a slightly more sophisticated version of the current evaluation. Instead of checking only Holoclean's most likely value, we can have it check if any of the k most likely values are the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change is done simply by specifying a parameter when initializing Holoclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-5 precision  is : 1.0\n",
      "The top-5 recall is : 1.0 out of 185\n",
      "The  MAP precision  is : 0.997719869707\n",
      "The MAP recall is : 0.978378378378 out of 185\n"
     ]
    }
   ],
   "source": [
    "holo.spark_session.stop()\n",
    "\n",
    "holo_2 = HoloClean(holoclean_path=\"..\", k_inferred = 5)\n",
    "session = Session(holo_2)\n",
    "\n",
    "data = session.load_data(data_path)\n",
    "dcs = session.load_denial_constraints(dc_path)\n",
    "\n",
    "detector = SqlDCErrorDetection(session)\n",
    "\n",
    "error_detector_list =[]\n",
    "error_detector_list.append(detector)\n",
    "clean, dirty = session.detect_errors(error_detector_list)\n",
    "\n",
    "repaired = session.repair()\n",
    "\n",
    "session.compare_to_truth(\"data/hospital_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working on more sophisticated evaluation techniques, including an interactive evaluator that enables users to self-examine a sample of the inferred values, so stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

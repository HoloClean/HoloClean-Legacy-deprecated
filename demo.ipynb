{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Step-by-Step Guide to Holoclean example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy and erroneous data is a major bottleneck in analytics. Data cleaning and repairing account for about 60% of the work of data scientists. To address this bottleneck, we recently introduced HoloClean, a semi-automated data repairing framework that relies on statistical learning and inference to repair errors in structured data. In HoloClean, we build upon the paradigm of weak supervision and demonstrate how to leverage diverse signals, including user-defined heuristic rules (such as generalized data integrity constraints) and external dictionaries, to repair erroneous data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we walk through the process of implementing Holoclean, by creating a simple end-to-end example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This post is an executable Jupyter notebook: youâ€™re encouraged to download it and experiment with the examples yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import all the module from Holoclean that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from holoclean.holoclean import HoloClean, Session\n",
    "from holoclean.errordetection.errordetector import ErrorDetectors\n",
    "from holoclean.featurization.featurizer import Signal_Init,Signal_cooccur, Signal_dc\n",
    "from holoclean.learning.accuracy import Accuracy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we create the Holoclean and Session object that we will use for this example. In addition the user gives the position for input file and the file that contains the denial constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmichalo/anaconda/envs/python27/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/base.py:1576: Warning: '@@tx_isolation' is deprecated and will be removed in a future release. Please use '@@transaction_isolation' instead\n",
      "  cursor.execute('SELECT @@tx_isolation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time of this part was: 57.4448869228 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_dd = time.time()\n",
    "holo_obj = HoloClean()\n",
    "session = Session(\"Session\", holo_obj)\n",
    "#csv_path=raw_input(\"Please give the full path for the csv file:\")\n",
    "csv_path=\"test/test.csv\"\n",
    "session.ingest_dataset(csv_path)\n",
    "#dc_path=raw_input(\"Please give the full path for the file of the denial constraints:\")\n",
    "dc_path=\"test/dc1.txt\"\n",
    "session.denial_constraints(dc_path)\n",
    "finish_time_dd = time.time()\n",
    "print(\"The running time of this part was: \"+ str((finish_time_dd-start_time_dd)) +\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Error detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we create the error detection. The output of this part is the C_dk table that contains all the noisy cells and the C_Clean table that contains the clean cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time of this part was: 114.105597019 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_dd = time.time()\n",
    "\n",
    "err_detector = ErrorDetectors(session.Denial_constraints, holo_obj.dataengine,\n",
    "                                      holo_obj.spark_session, session.dataset)\n",
    "session.add_error_detector(err_detector)\n",
    "session.ds_detect_errors()\n",
    "\n",
    "finish_time_dd = time.time()\n",
    "print(\"The running time of this part was: \"+ str((finish_time_dd-start_time_dd)) +\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we prune the domain. The output of this part is the possible_values tables that contains all the possible values for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time of this part was: 3.64925289154 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_dd = time.time()\n",
    "\n",
    "pruning_threshold=0.5\n",
    "session.ds_domain_pruning(pruning_threshold)\n",
    "\n",
    "finish_time_dd = time.time()\n",
    "print(\"The running time of this part was: \"+ str((finish_time_dd-start_time_dd)) +\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we implement the featurization module of holoclean. We choose the signals that we want to use and the output of this part is the featurization table that contains the factors that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time of this part was: 0.773250102997 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_dd = time.time()\n",
    "\n",
    "initial_value_signal = Signal_Init(session.Denial_constraints, holo_obj.dataengine,\n",
    "                                           session.dataset)\n",
    "session.add_featurizer(initial_value_signal)\n",
    "statistics_signal = Signal_cooccur(session.Denial_constraints, holo_obj.dataengine,\n",
    "                                           session.dataset)\n",
    "session.add_featurizer(statistics_signal)\n",
    "dc_signal = Signal_dc(session.Denial_constraints, holo_obj.dataengine, session.dataset)\n",
    "session.add_featurizer(dc_signal)\n",
    "session.ds_featurize()\n",
    "\n",
    "finish_time_dd = time.time()\n",
    "print(\"The running time of this part was: \"+ str((finish_time_dd-start_time_dd)) +\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the learning phase, we create a wrapper for numbskull that we will use for the gibbs sampling. The output of this part is the new weight table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time of this part was: 3.92434692383 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_dd = time.time()\n",
    "\n",
    "session._numskull()\n",
    "\n",
    "finish_time_dd = time.time()\n",
    "print(\"The running time of this part was: \"+ str((finish_time_dd-start_time_dd)) +\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we use the new weight, to learn the probabilities for each value for the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time of this part was: 1.01248002052 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_dd = time.time()\n",
    "\n",
    "session.ds_repair()\n",
    "\n",
    "finish_time_dd = time.time()\n",
    "print(\"The running time of this part was: \"+ str((finish_time_dd-start_time_dd)) +\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we find the accuracy of our results by comparing them to the clean version of our initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision that we have is :0.75\n",
      "The recall that we have is :0.75\n",
      "The F1 score that we have is :0.75\n",
      "The running time of this part was: 12.4441890717 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_dd = time.time()\n",
    "\n",
    "#csv_path_correction=raw_input(\"Please give the full path for the csv file for testing the accuracy:\")\n",
    "csv_path_correction=\"test/testGT.csv\"\n",
    "acc = Accuracy(holo_obj.dataengine, csv_path_correction, session.dataset, holo_obj.spark_session)\n",
    "acc.accuracy_calculation()\n",
    "\n",
    "finish_time_dd = time.time()\n",
    "print(\"The running time of this part was: \"+ str((finish_time_dd-start_time_dd)) +\" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
